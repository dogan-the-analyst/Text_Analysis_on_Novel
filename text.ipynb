{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pull text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textfile = open(\"pg1400.txt\", \"r\", encoding=\"utf-8\")\n",
    "great_expect = textfile.read()\n",
    "\n",
    "print(great_expect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "%pip install nltk\n",
    "%pip install gensim\n",
    "%pip install wordcloud\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import ldamodel\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download(\"punkt\")\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"vader_lexicon\")\n",
    "nltk.download(\"wordnet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lowercase words for word cloud\n",
    "word_cloud_text = great_expect.lower()\n",
    "# Remove numbers and alphanumeric words we don't need for word cloud\n",
    "word_cloud_text = re.sub(\"[^a-zA-Z0-9]\", \" \", word_cloud_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the data to split it into words\n",
    "tokens = word_tokenize(word_cloud_text)\n",
    "# Remove stopwords\n",
    "tokens = (word for word in tokens if word not in stopwords.words(\"english\"))\n",
    "# Remove short words less than 3 letters in length\n",
    "tokens = (word for word in tokens if len(word) >= 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data cleaning to split data into sentences\n",
    "alphabets = \"([A-Za-z])\"\n",
    "prefixes = \"(Mr|St|Mrs|Ms|Dr)[.]\"\n",
    "suffixes = \"(Inc|Ltd|Jr|Sr|Co)\"\n",
    "starters = r\"(Mr|Mrs|Ms|Dr|Prof|Capt|Cpt|Lt|He\\s|She\\s|It\\s|They\\s|Their\\s|Our\\s|We\\s|But\\s|However\\s|That\\s|This\\s|Wherever)\"\n",
    "acronyms = \"([A-Z][.][A-Z][.](?:[A-Z][.])?)\"\n",
    "websites = \"[.](com|net|org|io|gov|edu|me)\"\n",
    "digits = \"([0-9])\"\n",
    "\n",
    "text = \" \" + great_expect + \"  \"\n",
    "text = text.replace(\"\\n\",\" \")\n",
    "text = re.sub(prefixes,\"\\\\1<prd>\",text)\n",
    "text = re.sub(websites,\"<prd>\\\\1\",text)\n",
    "text = re.sub(digits + \"[.]\" + digits,\"\\\\1<prd>\\\\2\",text)\n",
    "if \"...\" in text: text = text.replace(\"...\",\"<prd><prd><prd>\")\n",
    "if \"Ph.D\" in text: text = text.replace(\"Ph.D.\",\"Ph<prd>D<prd>\")\n",
    "text = re.sub(r\"\\s\" + alphabets + \"[.] \",\" \\\\1<prd> \",text)\n",
    "text = re.sub(acronyms+\" \"+starters,\"\\\\1<stop> \\\\2\",text)\n",
    "text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\\\\3<prd>\",text)\n",
    "text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\",text)\n",
    "text = re.sub(\" \"+suffixes+\"[.] \"+starters,\" \\\\1<stop> \\\\2\",text)\n",
    "text = re.sub(\" \"+suffixes+\"[.]\",\" \\\\1<prd>\",text)\n",
    "text = re.sub(\" \" + alphabets + \"[.]\",\" \\\\1<prd>\",text)\n",
    "if \"”\" in text: text = text.replace(\".”\",\"”.\")\n",
    "if \"\\\"\" in text: text = text.replace(\".\\\"\",\"\\\".\")\n",
    "if \"!\" in text: text = text.replace(\"!\\\"\",\"\\\"!\")\n",
    "if \"?\" in text: text = text.replace(\"?\\\"\",\"\\\"?\")\n",
    "text = text.replace(\".\",\".<stop>\")\n",
    "text = text.replace(\"?\",\"?<stop>\")\n",
    "text = text.replace(\"!\",\"!<stop>\")\n",
    "text = text.replace(\"<prd>\",\".\")\n",
    "sentences = text.split(\"<stop>\")\n",
    "sentences = [s.strip() for s in sentences]\n",
    "sentences = pd.DataFrame(sentences)\n",
    "sentences.columns = ['sentence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out sentences variable\n",
    "print(len(sentences))\n",
    "print(sentences.head(59))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the first few rows of text that are irrelevant for analysis\n",
    "sentences.drop(sentences.index[:59], inplace=True)\n",
    "sentences = sentences.reset_index(drop=True)\n",
    "sentences.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create word cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_wc = set(stopwords.words(\"english\"))\n",
    "\n",
    "wordcloud = WordCloud(max_words=100, stopwords=stopwords_wc, random_state=1).generate(word_cloud_text)\n",
    "plt.figure(figsize=(12,16))\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Improve a word cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define gray_color_func function and mask variable for advanced word cloud\n",
    "mask = np.array(Image.open(\"man_in_top_hat.jpeg\"))\n",
    "\n",
    "def gray_color_func(word, font_size, position, orientation, random_state=None, **kwargs):\n",
    "    return \"hsl(0, 0%%, %d%%)\" % random.randint(60, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create advanced word cloud with our text data\n",
    "wordcloud = WordCloud(background_color=\"purple\", max_words=100, mask=mask, color_func=gray_color_func, stopwords=stopwords_wc, random_state=1).generate(word_cloud_text)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyze word frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create word frequency distribution\n",
    "fdist = nltk.FreqDist(tokens)\n",
    "fdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the 50 most common words in the text\n",
    "fdist.most_common(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of top 50 most common words in text\n",
    "plt.figure(figsize=(12,6))\n",
    "fdist.plot(50)\n",
    "plt.xticks(rotation = 80)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of top 50 most common words in text cumulatively\n",
    "plt.figure(figsize=(12,6))\n",
    "fdist.plot(50,cumulative=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perform Vader sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Vader sentiment analyzer\n",
    "analyzer = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform Vader sentiment analysis\n",
    "sentences[\"compound\"] = [analyzer.polarity_scores(x)[\"compound\"] for x in sentences[\"sentence\"]]\n",
    "sentences[\"neg\"] = [analyzer.polarity_scores(x)[\"neg\"] for x in sentences[\"sentence\"]]\n",
    "sentences[\"neu\"] = [analyzer.polarity_scores(x)[\"neu\"] for x in sentences[\"sentence\"]]\n",
    "sentences[\"pos\"] = [analyzer.polarity_scores(x)[\"pos\"] for x in sentences[\"sentence\"]] \n",
    "sentences.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get number of positive, neutral, and negative sentences\n",
    "positive_sentence = sentences.loc[sentences[\"compound\"] > 0]\n",
    "negative_sentence = sentences.loc[sentences[\"compound\"] < 0]\n",
    "neutral_sentence = sentences.loc[sentences[\"compound\"] == 0]\n",
    "\n",
    "print(sentences.shape)\n",
    "print(\"Positive: \" + str(len(positive_sentence)))\n",
    "print(\"Negative: \" + str(len(negative_sentence)))\n",
    "print(\"Neutral: \" + str(len(neutral_sentence)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Vader sentiment results \n",
    "plt.figure(figsize=(14,6))\n",
    "plt.hist(sentences[\"compound\"], bins=50);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perform topic modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert sentence data to list\n",
    "data = sentences[\"sentence\"].values.tolist()\n",
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text cleaning and tokenization using function\n",
    "def text_processing(texts):\n",
    "    # Remove numbers and alphanumerical words we don't need\n",
    "    texts = [re.sub(r\"[^a-zA-Z]+\", \" \", str(text)) for text in texts]\n",
    "    # Tokenize & lowercase each word\n",
    "    texts = [[word for word in text.lower().split()] for text in texts]\n",
    "    # Stem each word\n",
    "    lmtzr = WordNetLemmatizer()\n",
    "    texts = [[lmtzr.lemmatize(word) for word in text] for text in texts]\n",
    "    # Remove stopwords\n",
    "    stoplist = stopwords.words('english')\n",
    "    texts = [[word for word in text if word not in stoplist] for text in texts]\n",
    "    # Remove short words less than 3 letters in length\n",
    "    texts = [[word for word in tokens if len(word) >= 3] for tokens in texts]\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply function to process data and convert to dictionary\n",
    "data = text_processing(data)\n",
    "dictionary = Dictionary(data)\n",
    "len(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create corpus for LDA analysis\n",
    "corpus = [dictionary.doc2bow(text) for text in data]\n",
    "len(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perform topic model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find optimal k value for the number of topics for our LDA analysis\n",
    "np.random.seed(1)\n",
    "k_range = range(6,20,2)\n",
    "scores = []\n",
    "\n",
    "for k in k_range:\n",
    "    LdaModel = ldamodel.LdaModel(corpus=corpus, id2word=dictionary, num_topics=k, passes=20)\n",
    "    cm = CoherenceModel(model=LdaModel, corpus=corpus, dictionary=dictionary, coherence=\"u_mass\")\n",
    "\n",
    "    print(cm.get_coherence())\n",
    "    \n",
    "    scores.append(cm.get_coherence())\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(k_range, scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LDA topic model\n",
    "model = ldamodel.LdaModel(corpus, id2word=dictionary, num_topics=6, passes=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print topic distribution\n",
    "model.show_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
